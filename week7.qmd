---
title: "Week 7:  Parameterization"
---

# Tuesday

We develop hypotheses is science to test them.  Testing them requires a few steps before we can really dive into tests.  First, we must estimate the unknowns.  That process of estimation is called parameterization.  What do I mean by parameters?  I mean the unknowns in our model.  For example, in a simple, bivariate (one predictor, one outcome) linear regression model, we have two parameters:  the intercept and the slope.  We must estimate these parameters before we can test our hypotheses OR make predictions.  We use the data to estimate these parameters.  We then use these estimates to test our hypotheses and predict future events.  Before hypothesis testing and prediction, we must parameterize our model.  This is the process of estimation.

## Central Tendency

$$\mu = \frac{\sum_{i=1}^n x_i}{n}$$

## Dispersion

$$\sigma^2 = \frac{SS}{df} =\frac{\sum_{i=1}^n (x_i - \mu)^2}{n-1}$$

## Coefficients

$$\beta = \frac{\sum_{i=1}^n (x_i - \mu)(y_i - \nu)}{\sum_{i=1}^n (x_i - \mu)^2}$$

# The Logic Here

We must test our data to first summarize it correctly - a process we refer to as model fitting or parameterization.  In the process of parameterization, we estimate the unknowns in our model.  We then use these estimates to test our hypotheses.  This is the process of hypothesis testing.  We will cover this in more detail in the coming weeks.  For now, we focus on parameterization and estimation.  


## An example

```{r Example}
library(ggplot2)
library(dplyr)
library(brms)
library(shinystan)

data(mtcars)
lm1 <- lm(mpg~disp*as.factor(cyl), data=mtcars)
summary(lm1)

lm1.brms <- brm(lm1, data=mtcars)
shinystan::launch_shinystan(lm1.brms)
```




# Thursday

We will continue our discussion of parameterization and estimation.  We will also discuss the logic of hypothesis testing and prediction

## Parameters

1. Central Tendency
2. Dispersion
3. Coefficients
4. Errors
5. Effect sizes
6. Confidence Intervals
7. Prediction Intervals

## Some Considerations

Think of central tendency as a way for us to get the best guesses of our data.

$$Y = \hat{Y} + \epsilon$$
The guesses we make are deterministic (i.e., they are affected by data).  The errors are stochastic (i.e., they are not affected by data).  We can think of our observations as the sum of these two components.

$$\text{Observations} = \text{Deterministic Component} + \text{Stochastic Component}$$
The logic here is that we can make the best guesses we can, but there will always be some error in our guesses.  This is the nature of the world.  We can never know everything.  We can only make the best guesses with the available data - just the nature of science and life.  We can never know the truth; we can only make the best guesses available and quantify our uncertainty.

$$Y_{Obs} = \hat{Y}_{Det} + \epsilon_{Stoch}$$

We shall talk about this today.