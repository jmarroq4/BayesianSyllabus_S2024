# Week 2

If you are seeing this page, you successfully rendered the syllabus and week 1 notes. Congratulations! Now, we have work to do. This week, we will be learning about the following topics:

## Some Administration Details

### Maintaining your Rig

1.  [Maintain Quarto (hardest due to almost daily updates)](https://github.com/rocker-org/rocker-versioned2/blob/master/scripts/install_quarto.sh "Use this script")
2.  Maintain RStudio (relatively easy, weekly updates)
3.  Maintain R (easy, quarterly updates)

```{r version-check, eval=FALSE, echo=T, message=FALSE, warning=FALSE, collapse=T}
library(jsonlite)

if (Sys.which("wget") != "") {
  print("You have wget.  Getting Quarto version.")
  download.file("https://quarto.org/docs/download/_download.json", 
                destfile ="tmp.json", method="wget")
  quartoRC <- fromJSON("tmp.json")
  quartoRC$version
  download.file("https://quarto.org/docs/download/_prerelease.json", 
                destfile ="tmp2.json", method="wget")
  quartoPRE <- fromJSON("tmp2.json")
  quartoPRE$version
} else {
  print("Sorry, no wget or curl.  Please check Quarto versions manually.")
}

```

::: {#wk2versions .callout-important title="Latest Versions"}
Check Rstudio version and R version EVERY week. As of today (`r date()`), the versions are:

| Software         | Latest Version |
|------------------|----------------|
| R                | 4.3.2          |
| RStudio          | 2023-12.0-369  |
| Quarto (current) | 1.3.450        |
| Quarto RC        | 1.4.547        |

: Latest and Greatest (Stable) Versions
:::

## Why Bayesian Statistics?

The world according to the frequentist (or by xkcd):

[![Which are you?](https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png){fig-align="center"}](https://xkcd.com/1132)

[Not the best illustration](https://stats.stackexchange.com/questions/43339/whats-wrong-with-xkcds-frequentists-vs-bayesians-comic "Read on McDuff!")

[A better illustration requires some explanation (pages 9-22)](https://faculty.washington.edu/kenrice/BayesIntroClassEpi2018.pdf "See Pages 9-onward")

### A probability problem

A frequentist gets a p-value from a statistical procedure. That p-value represents the following probability:

$$p-value_{freq} = P(D | H_0) $$

where $P()$ stands for probability of the stuff inside the parentheses, $D$ stands for the data (or results) and $H_0$ is the null hypothesis (i.e., there are no results worth talking about because the null is zero). The pipe "$|$" is the sign for "given" and separates the focal interest (the results) from the conditions. Thus, the p-value is a conditional probability but for something we generally do not wish to check. Instead, we scientists are interested in hypothesis testing. Why? Because we want to actually assess the probability our hypothesis is true given the results we obtained. We even think that way with frequentist statistics. Unfortunately, the frequentist's p-value does not tell us what we want to know.

### A hypothesis testing problem

So, we have a problem with our probabilities. We want to know the probability of the hypothesis given the data or:

$$p-value_{new} = P(H|D)$$

where now we have the same elements but they are switched. Instead of obtaining the conditional probability of the data given the hypothesis, we get the conditional probability of the hypothesis given the data. Whats more, we get the probability of *our* or rather *any* hypothesis given the data; we no longer get restricted to the null hypothesis. Getting that probability is the goal of Bayesian statistics. We will learn how to do that in this course.

## The Bayesian Solution

Bayes' Theorem is our solution to the problem here and is defined as follows:

$$p-value_{Bayes} = P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$

where $P(H|D)$ is the probability of the hypothesis given the data, $P(D|H)$ is the probability of the data given the hypothesis, $P(H)$ is the probability of the hypothesis, and $P(D)$ is the probability of the data. The denominator is the probability of the data and is calculated as follows:

$$P(D) = \sum_{i=1}^{n} P(D|H_i)P(H_i)$$

where $n$ is the number of hypotheses. This is the probability of the data given any hypothesis times the probability of that hypothesis. The sum of these products is the probability of the data. We will learn how to calculate this probability in this course.

## Distributions

These conditional probabilities are not point estimates as is often viewed by users (not experts) of frequentist statistics. Nope, the $P(H|D)$ is not a point estimate. Instead, we Bayesians think of the probability not as a single value but rather as a distribution of possible values. That distribution of possible values needs to be learned via direct exposure to data and distributions - both from a real data and a theoretical perspective. These distributions communicate a lot more information than single values. We will learn about distributions in this course because that knowledge will help you model the world that you wish to study. We will learn about the following distributions:

### The Normal Distribution

You think you know the normal distribution because that is what you learned in all your statistics classes to date. Well, you do know the normal distribution but you do not know it well enough. How do I know this? Well, I don't; I am using my prior experiences with graduate students to "color" my perceptions of this forecast. So, I am acting like a Bayesian here. What is it that you are missing? You are missing the details.

```{r, echo=T, message=FALSE, warning=FALSE, collapse=T}

# The Normal Distribution
library(tidyverse)
library(extrafont)
library(ggplot2)
library(xkcd)
df <- tibble(x = seq(-4, 4, length.out = 100), y = dnorm(x, mean = 0, sd = 1))
p1 <- ggplot(df, aes(x = x, y = y)) + geom_line() + labs(x = "Some arbitrary variable x (Standardized)", y = "Density of variable y", title = "The Standard Normal Distribution")

if( 'xkcd' %in% fonts()) {
  p1 + theme_xkcd()
} else {
  warning("No xkcd fonts installed!")
  library(extrafont)
  download.file("http://simonsoftware.se/other/xkcd.ttf", dest="xkcd.ttf", mode="wb")
  system("mkdir ~/.fonts")
  system("cp xkcd.ttf ~/.fonts")
  font_import(pattern="[X/x]kcd", prompt=FALSE)
  fonts()
  fonttable()
  if(.Patform$OS.type != "unix") {
    # register fonts for windows bmp output
    loadfonts(device = "win")
  } else {
    loadfonts()
  }
  p1 + theme_xkcd()
}

```

The density plot above tells us a fair bit. But you need to understand how we go from a curve like that one to probabilities.

::: callout-note
During class on Tuesday, I intend to annotate the figure above. Those annotations can be easily redone via ggplot. If you care to learn how, ask in class.
:::

## The Beta Distribution

The normal distribution, while "normal" in the sense that it is quite normative in social and behavioral sciences, it is quite abnormal with respect to complexity. Compared to other distributions, the normal distribution is extremely complicated with respect to the innards (i.e., maths). A simpler, gentler, and more mathematically tractable for us arithmetically challenged, I present to you the beta distribution.

```{r}
# The Beta Distribution
library(tidyverse)
library(extrafont)
library(ggplot2)
df <- tibble(x = seq(0, 1, length.out = 100), y = dbeta(x, shape1 = .5, shape2 = .5))
p1 <- ggplot(df, aes(x = x, y = y)) + geom_line() + labs(x = "A range of probabilities", y = "Probability of said probability", title = "The Standard Beta Distribution")
p1 + theme_xkcd()
```

Change the shape1 and shape2 parameters in the code above and see what happens to the overall shape of the distribution. Here are a few values to try:

-   shape1 = 1, shape2 = 1
-   shape1 = 2, shape2 = 2
-   shape1 = 2, shape2 = 5
-   shape1 = 5, shape2 = 2
-   shape1 = 5, shape2 = 5

What happens?

The Beta distribution is a probability distribution that we will use to model probabilities - at least initially. It is a continuous distribution that is defined on the interval $[0,1]$ and by two parameters, $\alpha$ and $\beta$. For those math geeks among us, you can represent the function as:

$$Beta(\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1-x)^{\beta - 1}$$

where $\Gamma()$ is the gamma function, $x$ is the probability, and $\alpha$ and $\beta$ are the parameters. Furthermore, (more math stuffs here), the gamma function I referred to above is defined as:

$$\Gamma(x) = \int_{0}^{\infty} t^{x-1}e^{-t}dt$$

where $x$ is the parameter. The gamma function is a generalization of the factorial function. The factorial function is defined as follows:

$$n! = n(n-1)(n-2)\cdots(2)(1)$$

where $n$ is the parameter. The gamma function is a generalization of the factorial function because it allows for non-integer values of $x$.

### Why do you need to know this?

These shapes define our problem space. Each probability density function has meaning to us as an area to explore. We will learn how to apply these functions to a variety of problems in our own areas of inquiry.

### Interesting Probability Problems (for those chomping at the bit)

See Fred Mosteller's outstanding book: Fifty Challenging Problems in Probability with Solutions. It is a great book and will help you think about probability in a different way. Don't stress it though. The book is a resource for those who want to challenge their thinking about fundamental probability problems; it is not required reading for this course.

## Basic Probability

Probability came from gambling. People wanted to win or at least increase their chances of winning. The study of probability was born out of this desire. The study of probability is the study of chance and chance favors the prepared mind. We will prepare our minds by learning the basics of probability.

A probability is a number between 0 and 1. A probability of 0 means that the event will never occur. A probability of 1 means that the event will always occur. We calculate said value by a simple fraction.

$$
probability = \frac{number\ of\ outcomes\ an\ event\ can\ occur}{total\ number\ of\ possible\ outcomes}
$$

Most of us think of probability as a percentage. We can convert a probability to a percentage by multiplying the probability by 100. For example, the probability of rolling a 1 on a six-sided die is 1/6 or 0.1667. If we multiply 0.1667 by 100, we get 16.67%. Thus, we can estimate these probabilities as percentages and vice versa; regardless, we interpret them the same.

### Probability and Odds

[![](http://www.phdcomics.com/comics/archive/phd1105.gif){fig-align="center"}](https://phdcomics.com/comics/archive.php?comicid=8)

You will frequently see probabilities listed in two (perhaps even more) forms - as probabilities or odds ratios.  First, the probability.  Since we know that probabilities range between 0 and 1, we know that there must be a fraction of sorts.  Probabilities are simple fractions of the form:

$$
probability = \frac{part}{whole} = \frac{part}{part + not\ part}
$$
Second, we have odds ratios.  Odds ratios are also fractions, but they have different values.  Odds ratios are of the form:

$$
odds\ ratio = \frac{part}{not\ part}
$$

The difference between the two is that the denominator of the odds ratio is not the whole but rather the complement of the part we wish to estimate the probability.  These ratios can be a bit confusing, but it is important to understand the difference between the two.  The odds ratio is a ratio of the part to the not part.  The probability is a ratio of the part to the whole.  The two are related, but they are not the same.  Most of what we talk about in this course focuses on probabilities, but you will see odds ratios in the literature.  It is important to understand the difference between the two.

### Some Rules to "Live" by (or at least to help you understand probability)

1.  Range Rule: $0 \leq P(A) \leq 1$ where P(A) is the probability of event A.

    ::: callout-warning
    Ever see one of these?

    p \< 0.000\*\*\*

    Well, you have. They don't make sense either. Rounding error.
    :::

2.  Complement Rule (aka Law of Total Probability): $P(A) + P(A^c) = 1$ (Note: $A^c$ means the complement of A.)

    ::: callout-warning
    An event and all other non-events must add up to 1. This complement rule is a rule that puts a ton of power into your hands without you knowing it. Beware though! The complement $P(A^c)$ is not always readily available.
    :::

3.  Addition Rule: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ (Note: $A \cup B$ means the union of A and B. $A \cap B$ means the intersection of A and B.)

4.  Multiplication Rule: $P(A \cap B) = P(A)P(B|A)$ (Note: $P(B|A)$ means the probability of B given A.)

5.  Independence Rule: $P(A\ and\ B) = P(A \cap B) = P(A)P(B)$ (Note: A and B are independent if the probability of A is not affected by the probability of B and vice versa.)

[![](https://imgs.xkcd.com/comics/prediction.png){fig-align="center"}(https://xkcd.com/2370)

6.  Conditional Probability Rule: $P(A\ given\ B) = P(A|B) = \frac{P(A \cap B)}{P(B)}$ (Note: $P(A|B)$ means the probability of A given B.)

[![](https://imgs.xkcd.com/comics/seashell.png){fig-align="center"}](https://xkcd.com/1236)

### Some Examples

1.  What is the probability of rolling a 1 on a six-sided die?

    $$
    P(1) = \frac{1}{6} = 0.1667
    $$

2.  What is the probability of rolling a 1 or a 2 on a six-sided die?

    $$
    P(1\ or\ 2) = \frac{2}{6} = 0.3333
    $$
    ::: callout-info
    Note: The word "or" is a signal to add probabilities.
    :::
    
3.  What is the probability that you will roll a combined total of 7 on two six-sided dice?

    $$
    P(7) = \frac{6}{36} = 0.1667
    $$
    ::: callout-info
    Note: The word "and" is a signal to multiply probabilities.
    :::
    
4.  What is the probability that you will roll a 1 on the first die and a 2 on the second die?

    $$
    P(1\ and\ 2) = \frac{1}{36} = 0.0278
    $$
    ::: callout-info
    Note: The word "and" is a signal to multiply probabilities.
    :::
    
5.  What is the probability that you will roll a 1 on the first die given that you rolled a 2 on the second die?

    $$
    P(1\ given\ 2) = \frac{1}{6} = 0.1667
    $$
    ::: callout-info
    Note: The word "given" is a signal to use conditional probability.
    :::
    

    
