---
title: "Week 4"
format: html
server: shiny
editor: 
  markdown: 
    wrap: 72
---

## Project Tips

1.  Define the problem
2.  Generate/collect data
3.  Refine and test the model
4.  Evaluate the results
5.  Communicate

Each of you will have multiple roles throughout the semester. I assign
roles to each of you per project.

## The Beta Function (Revisited)

Remember ye ol Beta function? I mentioned last week that the beauty of
said function (i.e., a probability density function or pdf) is the
simplicity of updates. The Beta function is a conjugate prior to the
Binomial distribution. This means that the posterior distribution is
also a Beta distribution. This is a beautiful thing because it allows us
to update our prior beliefs with new data and all such updates are
readily communicated in the same metric - probabilities of
probabilities. A Beta probability density function often gets
communicated in the Bayesian literature as such:

$$\theta \sim \text{Beta}(\alpha, \beta)$$

where probabilities ($\theta$) can be simulated ($\sim$) for shape
values of $\alpha$ and $\beta$ using the Beta function. But what is this
Beta function? Thus, to mathify it, we have an unknown $\theta$ that can
be estimated from a set of possible unknowns ($\theta \in [0, 1]$) and
where the shape values of $\alpha$ and $\beta$ are greater than zero
($\alpha > 0; \beta > 0$). For simplicity, the $\alpha$ can be
interpreted as "successes" while $\beta$ can be interpreted as
"failures."

::: {.callout-warning title="Mathy Stuff" collapse="true"}
A function is often expressed as $f()$ and the innards of said function
get put inside the parentheses (the $()$ in the $f()$). What we put
inside matters too. So, let's start taking apart the function that
defines the probability density function.

$$ f(\theta; \alpha, \beta) = \frac{\theta^{\alpha - 1}(1 - \theta)^{\beta - 1}}{B(\alpha, \beta)} $$

Here, the $f(\theta; \alpha, \beta)$ says that beta can be expected to
behave accordingly; across $\theta$ (a set of binary outcomes;
$\theta \in [0,1]$ ), we can produce a density function (curve) for any
real, positive values of hits ($\alpha$) and misses ($\beta$) greater
than zero ($\alpha > 0; \beta > 0$).

Now that we got the players named, we need to step back and think like a
5th grader. The function above is a ratio:

$$
f(x) = \frac{Part}{Whole}
$$

The $Part$ here represents the bit we have at hand (what we know). The
$Whole$ is the entirety of the space we wish to compare the $Part$.
Thus, we have a ratio of a section over the entire area.

But wait! How can the $f()$ be defined by $\beta$ you ask? Of course you
asked that question. How? Well, the $f()$ above is a probability density
function that can be defined by a host of different shapes. The shape we
are looking at here is the $B(\alpha, \beta)$ in the denominator - the
real mover and shaker of this mathematical function.

$B(\alpha, \beta)$ is both the denominator AND the actual Beta function.
But really, the denominator is the area under the curve or, as we
discussed previously, the total probability space.

$$ B(\alpha, \beta) = \int_0^1 \theta^{\alpha - 1}(1 - \theta)^{\beta - 1} d\theta $$

Before your eyes glaze over, look at the math stuff above with a new
perspective. That curly thing with a zero at the bottom and a 1 at the
top just means "integral." We use that notation to alert the reader that
the material on the right hand side of the equation is an area (or
volume if you have more than 2 dimensions). For us, area suffices. We
wish to know the area under the curve between 0 and 1 or the entire
range of probability since probabilities much range between 0 and 1 (and
include them).

Integrals aside, we can also reform the beta function as a function of
another function (gamma). Here is the formulation:

$$ B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} $$

The new symbol ($\Gamma$) is worth a closer inspection. For those who
are familiar with enough basic arithmetic, read on!

$$ \Gamma(n) = (n - 1)! $$
$$ \Gamma(n) = \int_0^\infty x^{n - 1}e^{-x} dx $$
$$ \Gamma(n) = (n - 1)\Gamma(n - 1) $$
$$ \Gamma(\frac{1}{2}) = \sqrt{\pi} $$ $$ \Gamma(n + 1) = n\Gamma(n) $$

$$ \alpha = 1 + \text{number of heads} $$
$$ \beta = 1 + \text{number of tails} $$

$$ \theta \sim \text{Beta}(\alpha, \beta) $$
:::

## Updating the Beta Function

Bayes theorem allows us to update our beliefs with new evidence.

$$ \text{New Beliefs} \propto \text{Prior Beliefs} \times \text{Evidence} $$
Put more specifically with our new terminology, the posterior beliefs
are proportional to ($\propto$) the prior beliefs times the likelihood
of the evidence. In mathy terms, we can write this as such:

$$\text{Posteriors} \propto \text{Priors} \times \text{Likelihood}$$
and, in even more mathy terms, we can write:

$$ P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)} $$ You will soon
become so familiar with Bayes Theorem that you will be able to recite it
without thinking much about it. Just remember these simple things.

1.  Your new beliefs will always be directly proportional to your old
    beliefs. Thus:

$$P(\theta|X) \propto P(\theta)$$

2.  Your new beliefs will always be indirectly proportional to the
    collective probability of the evidence (all possible outcomes).
    Thus:

$$P(\theta|X) \propto \frac{1}{P(X)}$$ 3. The likelihood $P(X|\theta)$
just flips our bits and offers us the opposite of what we want
$P(\theta|X)$. Thus:

$$P(\theta|X) \neq P(X|\theta)$$

4.  Put simply, we take our old beliefs, multiply them by the likelihood
    of the evidence, and then normalize the result.

$$Posteriors = Priors \times Likelihood$$

Remember those tidbits and Bayes Theorem will be available without much
cognitive effort. Let's dig a bit deeper into these parts of Bayes
Theorem and show how they work with the Beta function.

### Prior Beliefs

Recall that I mentioned something quite rudimentary about Bayesian
statistics. We take our beliefs $\theta$ and assume they can be modeled
by some distribution - here, we use the $\beta$ function simply because
it IS SIMPLE. The distribution we choose is really important because it
forms the landscape of our thought process. Thus, we "mathematize" our
beliefs and, in so doing, we create the domain that we wish to travel in
order to learn. You see, Bayesian models are learning models. We begin
with beliefs.

Every belief can take any function form but mostly we try to select a
probability density function that matches what we are "believing." If
your domain of interest uses binary terms (yes/no, heads/tails, etc.),
then the Beta function is a good choice. Frequency counts? Gamma or
poisson may suffice. For our purposes, we begin with binary outcomes.
Researchers who study mortality (life/death), remission (sick/healthy),
or achievement (pass/fail) will find the Beta function to be quite
useful. Most of us who focus on more continuous outcomes need to look
elsewhere; regardless of your interests, the Beta distribution works for
illustrative purposes quite well. The Beta function is a distribution of
probabilities. It is a distribution of beliefs expressed in
probabilities for binary outcomes.

### Evidence

Empirical science is literally the use of observation to update our
understanding. These observations are the "evidence" used to change how
we view the world (or small slice of the world). The evidence we rely
upon is often in the form of data that can be transformed into a
suitable set of metrics that "mesh" with our beliefs. If I want to
change my beliefs that pertain to a binary outcome, I must make
observations that may be recorded as such. You are now familiar enough
with NHST to understand that we collect data to test the null. But, in
Bayesian statistics, we collect data to update our beliefs. If we relied
solely on our beliefs, science would be unnecessary.

### The Aftermath

The posterior beliefs are the result of the evidence and the prior
beliefs and form the basis of our new beliefs. We use these new beliefs
to make decisions, form new theories, or revise old theories. Beliefs
form, change, and get renewed with every study. Thus, we learn. The real
aftermath of these models is learning. Furthermore, the posteriors serve
as the priors for the next study. We are always learning and always
updating our beliefs. This is the essence of Bayesian statistics. We
learn.

## Beta in Action

So now we have the Beta function and the logic of Bayesian updating. The
real reason we begin with the Beta function is that it is simple and
easy to understand from a Bayesian updating perspective. What is
updating you ask? Glad you did. Updating is the process of combining the
priors and the evidence. We update our priors with evidence. The Beta
function is the EASIEST function to update. Let's see how this works.

::: {.callout-tip title="Beta Updater" appearance="simple" collapse="true"}
<!--# NOTE:  shiny apps will not work in callout right now.  2/4/2024 -->
Beta is the function du jour because it is easy to update.  Simply add the hits and misses together, respectively and you have your posterior shape parameters.  See?  Easy peasy lemon squeezy.

:::

## An Illustration

We have a new test to determine if a student knows statistics. That test
provides a binary outcome (pass/fail). We have a prior belief
($P(Knowledge) = .75$) that the student will pass the test. We have no
evidence to the contrary but we hold a high regard for our students.
After testing 10 students, we find that 7 pass and 3 fail. We want to
update our beliefs about the probability of passing the test. We will
use the Beta function to do this.

```{r}
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| panel: input
#| layout: [[33,33,33]]
library(shiny)
sidebarPanel("Priors")
sliderInput("pa","Hits",min=0,max=100,value=.5, step = .5, animate=F)
sliderInput("pb","Misses",min=0,max=100,value=.5, step=.5, animate=F)
```

```{r}
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| panel: sidebar
sidebarPanel("Evidence")
sliderInput("ea","Hits",min=0,max=100,value=0, step = .5, animate=F)
sliderInput("eb","Misses",min=0,max=100,value=0, step=.5, animate=F)
```



```{r}
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| panel: fill
plotOutput("myPlot")
```

```{r}
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| context: server
library(tidyverse)
output$myPlot <- renderPlot({
  ggplot() + 
    stat_function(fun=dbeta, args=list(shape1=as.numeric(input$pa),
                                       shape2=as.numeric(input$pb),
                                       ncp=0),
                  colour="green",
                  linetype="solid",
                  linewidth = 2) +
    stat_function(fun=dbeta, args=list(shape1=(as.numeric(input$pa) + 
                                                 as.numeric(input$ea)),
                                       shape2=(as.numeric(input$pb) + 
                                                 as.numeric(input$eb)),
                                       ncp=0),
                  colour="blue",
                  linetype="solid",
                  linewidth = 2) +
    xlim(0,1) +
    ylim(0,18)
})
```


## New Resources

1.  [Python approach to Option
    1](https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking)
2.  [Python approach to Option
    2](https://www.kaggle.com/code/bhavinmoriya/bayesian-statistics-the-fun-way-by-will-kurt)
